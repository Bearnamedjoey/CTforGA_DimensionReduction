<br> 
<center><img src="http://i.imgur.com/sSaOozN.png" width="500"></center>


## Course:  COMPUTATIONAL THINKING FOR GOVERNANCE ANALYTICS

### Prof. José Manuel Magallanes, PhD 
* Visiting Professor of Computational Policy at Evans School of Public Policy and Governance, and eScience Institute Senior Data Science Fellow, University of Washington.
* Professor of Government and Political Methodology, Pontificia Universidad Católica del Perú. 


_____
<a id='part1'></a>

# Dimensionality Reduction in R

As the name implies, we want to _reduce_ a set of variables into one (or two) that summarizes them. In this session we will practice two basic techniques:

* Cluster analysis.

* Factor analysis.

## Cluster Analysis

Simply speaking, this technique will organize the cases (rows) into a small set of groups, based on the information (the columns) available for each case. 

Let me bring back the data we prepared in Python:

```{r collecting, eval=TRUE}
rm(list = ls())
link='https://github.com/EvansDataScience/CTforGA_integrating/raw/main/demo_fragile.RDS'
# a RDS file from the web needs:
myFile=url(link)

# reading in data:
fromPy=readRDS(file = myFile)

# reset indexes to R format:
row.names(fromPy)=NULL

# check data types
str(fromPy)
```

### I. Prepare data:

We have several countries, and several columns. In clustering, we try to create groups so that we have the highest homogeneity within groups, and the highest heterogeneity between groups. The variables will serve to try to find the homogeneity and heterogeneity mentioned, that is, some **distance** among them. Let's do that first:

1. Subset the data: For this case just keep the columns with numeric values without the categories or the summary scores:
```{r}
dataToCluster=fromPy[,-c(1:3,9)]
row.names(dataToCluster)=fromPy$Country
```

2. Decide if standardizing is needed: Take a look at the data ranges:

```{r}
summary(dataToCluster)
```
```{r boxplotS, eval=TRUE}
boxplot(dataToCluster,horizontal = T, las=2,cex.axis=0.4)
```

The data values have a similar range, then you do not need to transform the data. Possible alternatives could have been:

```{r transforming, eval=FALSE}
library(BBmisc)
#### standardizing
normalize(dataToCluster,method='standardize')

### normalizing
normalize(dataToCluster,method='range',range=c(0,10))
```



### II. Compute the DISTANCE MATRIX:


1. Set random seed:
```{r clusterSeed, eval=TRUE}
set.seed(999) # this is for replicability of results
```


2. Decide distance method and compute distance matrix:
```{r cluster_DistanceMatrix, eval=TRUE}
library(cluster)
distancesData=daisy(x=dataToCluster, metric = "gower")
```

3. Represent distances


We can prepare a bidimensional map. The function *cmdscale* can produce a two dimension map of points using the *distance matrix*:

```{r cmd_Map, eval=TRUE}
projectedData = cmdscale(distancesData, k=2)
```

The object _projectedData_ is saving coordinates for each element in the data:

```{r, eval=TRUE}
#
# save coordinates to original data frame:
fromPy$dim1 = projectedData[,1]
fromPy$dim2 = projectedData[,2]

# see some:

fromPy[,c('dim1','dim2')][1:10,]
```


* Use those points and see the "map":

```{r plotCmdmap, eval=TRUE}
library(ggplot2)
base= ggplot(data=fromPy,
             aes(x=dim1, y=dim2,
                 label=Country)) 
base + geom_text(size=2)
```

Can you see some groups emerging?

An alternative is the dendogram:

```{r}
# prepare hierarchical cluster
hc = hclust(distancesData)
# very simple dendrogram
plot(hc,hang = -1,cex=0.5)
```



## Compute Clusters

### 0. Computer suggestions

Using function *fviz_nbclust* from the library *factoextra* we can see how many clustered are suggested.

a. For hierarchical (agglomerative):

```{r, eval=TRUE}
library(factoextra)
fviz_nbclust(dataToCluster, 
             hcut,
             diss=distancesData,
             method = "gap_stat",
             k.max = 10,
             verbose = F,
             hc_func = "agnes")


```

c. For hierarchical (divisive):

```{r, eval=TRUE}
fviz_nbclust(dataToCluster, 
             hcut,
             diss=distancesData,
             method = "gap_stat",
             k.max = 10,
             verbose = F,
             hc_func = "diana")
```

We could accept the number of cluster suggested or not. Let's use the suggestion:

### 1. Apply function: you need to indicate a priori the amount of clusters required.

```{r clusterComputeALL, eval=TRUE}

NumberOfClusterDesired=4


#library(factoextra)
res.agnes= hcut(distancesData, 
                k = NumberOfClusterDesired,
                isdiss=TRUE,
                hc_func='agnes',
                hc_method = "ward.D2")

# Hierarchical technique- divisive approach
res.diana= hcut(distancesData, 
                k = NumberOfClusterDesired,
                isdiss=TRUE,
                hc_func='diana',
                hc_method = "ward.D2")
```


### 2. Clustering results. 

**2.1 ** Add results to original data frame:

```{r clusterSave_toDF, eval=TRUE}

fromPy$agn=as.factor(res.agnes$cluster)
fromPy$dia=as.factor(res.diana$cluster)
```

**2.2 ** Verify ordinality in clusters


```{r clusAgnCheck, eval=TRUE}
aggregate(data=fromPy,
          Overallscore~agn,
          FUN=mean)
```
```{r clusDiaCheck, eval=TRUE}
aggregate(data=fromPy,
          Overallscore~dia,
          FUN=mean)
```

You could recode these values like this:

```{r recoding, eval=TRUE}
library(dplyr)

fromPy$agn=dplyr::recode_factor(fromPy$agn, 
                  `1` = '4',`2`='3',`3`='2',`4`='1')
fromPy$dia=dplyr::recode_factor(fromPy$dia, 
                  `1` = '4',`2`='3',`3`='2',`4`='1')
```

### 3. Evaluate Results.

**3.1** Plot silhouettes


```{r clust_silhou_AGNES, eval=TRUE}

fviz_silhouette(res.agnes)
```

```{r clust_silhou_DIANA, eval=TRUE}
library(factoextra)
fviz_silhouette(res.diana)
```

**3.2** Detecting cases badly clustered

a. Save individual silhouettes:

Previos results have saved important information. Let me keep the negative sihouette values:

```{r negativeSILs, eval=TRUE}
agnEval=data.frame(res.agnes$silinfo$widths)
diaEval=data.frame(res.diana$silinfo$widths)

agnPoor=rownames(agnEval[agnEval$sil_width<0,])
diaPoor=rownames(diaEval[diaEval$sil_width<0,])
```

Now, I can see what countries are not well clustered:

```{r, eval=TRUE}
library("qpcR") 
bad_Clus=as.data.frame(qpcR:::cbind.na(sort(agnPoor),
                                       sort(diaPoor)))
names(bad_Clus)=c("agn","dia")
bad_Clus
```


# How to compare clustering?



* Color the map using the labels from Hierarchical AGNES:
```{r plotagn, eval=TRUE}
base= ggplot(data=fromPy,
             aes(x=dim1, y=dim2,
                 label=Country)) 
agnPlot=base + labs(title = "AGNES") + geom_point(size=2,
                                              aes(color=agn),
                                              show.legend = T) 
```


* Color the map using the labels from  Hierarchical DIANA:
```{r plotdia, eval=TRUE}
diaPlot=base + labs(title = "DIANA") + geom_point(size=2,
                                              aes(color=dia),
                                              show.legend = T) 
```


Compare visually:

```{r plotcompare, eval=TRUE}
library(ggpubr)

ggarrange(agnPlot, diaPlot,ncol = 2,common.legend = T)
```

* Annotating outliers:

Prepare labels:
```{r, eval=TRUE}
# If name of country in black list, use it, else get rid of it
LABELdia=ifelse(fromPy$Country%in%diaPoor,fromPy$Country,"")
LABELagn=ifelse(fromPy$Country%in%agnPoor,fromPy$Country,"")
```


```{r plotdb_annot2, eval=TRUE}
library(ggrepel)
diaPlot + geom_text_repel(aes(label=LABELdia),max.overlaps = 50,min.segment.length = unit(0, 'lines'))
```
```{r, eval=TRUE}
agnPlot + geom_text_repel(aes(label=LABELagn),max.overlaps = 50,min.segment.length = unit(0, 'lines'))
```

* The Dendogram (for hierarchical approaches)


```{r clusagnREPORTdendo, eval=TRUE}
fviz_dend(res.agnes,k=NumberOfClusterDesired, cex = 0.45, horiz = T,main = "AGNES approach")
```


```{r clusdiaREPORTdendo, eval=TRUE}
fviz_dend(res.diana,k=NumberOfClusterDesired, cex = 0.45, horiz = T,main = "DIANA approach")
```

Let's compare these clusters with the levels proposed by The Economist:
```{r}
table(fromPy$Regimetype,fromPy$agn)
```

```{r}
table(fromPy$Regimetype,fromPy$dia)
```


# <font color="red">FACTOR ANALYSIS</font>

Simply speaking, this technique tries to express in one (or few) dimension(s) the behavior of several others. FA assumes that the several input variables have 'something' in common, there is something **latent** that the set of input variables represent. 


Let me subset our original data frame:

```{r, eval=TRUE}

dataForFA=dataToCluster
```


Our *dataForFA* data frame has the data to compute the one index we need. I will show the technique called **confirmatory factor analysis**:

```{r, eval=TRUE}
names(dataForFA)
```
```{r}
library(polycor)
corMatrix=polycor::hetcor(dataForFA)$correlations
```
```{r}
library(ggcorrplot)

ggcorrplot(corMatrix,
           type = "lower") + 
          theme(axis.text.x  = element_text(size = 5),
                axis.text.y  = element_text(size = 5))
```

```{r}
library(psych)
psych::KMO(corMatrix) 
```

```{r}
cortest.bartlett(corMatrix,n=nrow(dataForFA))$p.value>0.05
```
```{r}
library(matrixcalc)

is.singular.matrix(corMatrix)
```
```{r}
fa.parallel(dataForFA, fa = 'fa',correct = T,plot = F)
```
```{r}
library(GPArotation)
resfa <- fa(dataForFA,
            nfactors = 2,
            cor = 'mixed',
            rotate = "varimax",
            fm="minres")
print(resfa$loadings)
```

```{r}
print(resfa$loadings,cutoff = 0.5)
```

```{r}
fa.diagram(resfa,main = "EFA results")
```

```{r}
sort(resfa$communality)
```


```{r}
sort(resfa$complexity)
```
```{r}
ps=c("P1StateLegitimacy","P2PublicServices","P3HumanRights")
notPs=setdiff(names(dataForFA),ps)
dataForFA=dataForFA[,notPs]
```

```{r}
# esta es:
library(polycor)
corMatrix=polycor::hetcor(dataForFA)$correlations
```


```{r}
library(psych)
psych::KMO(corMatrix) 
```

```{r}
cortest.bartlett(corMatrix,n=nrow(dataForFA))$p.value>0.05
```
```{r}
library(matrixcalc)

is.singular.matrix(corMatrix)
```
```{r}
fa.parallel(dataForFA, fa = 'fa',correct = T,plot = F)
```
```{r}
library(GPArotation)
resfa <- fa(dataForFA,
            nfactors = 2,
            cor = 'mixed',
            rotate = "varimax",
            fm="minres")
print(resfa$loadings)
```

```{r}
print(resfa$loadings,cutoff = 0.5)
```

```{r}
fa.diagram(resfa,main = "EFA results")
```

```{r}
sort(resfa$communality)
```


```{r}
sort(resfa$complexity)
```

```{r}
library(BBmisc)
efa_scores=normalize(resfa$scores, 
                       method = "range", 
                       margin=2, # by column
                       range = c(0, 10))

fromPy$Fragile_efa=efa_scores[,1]
fromPy$Demo_efa=efa_scores[,2]

library("ggpubr")
ggscatter(data=fromPy, x = "Overallscore", y = "Demo_efa", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "DemoIndex (original)", ylab = "DemoIndex (efa)")
```

```{r}


ggscatter(data=fromPy, x = "Total", y = "Fragile_efa", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "FragileIndex (original)", ylab = "FragileIndex (efa)")
```

